{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5628a8ad-4032-4f68-af55-8fd4be3ddc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "import whisper\n",
    "from transformers import pipeline\n",
    "import os\n",
    "import uvicorn\n",
    "import warnings\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e728b2a1-a321-44da-8ada-d915359c9d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", message=\"FP16 is not supported on CPU; using FP32 instead\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Using a GPU for inference is not supported; using CPU instead\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "nest_asyncio.apply() #to run Fast API on jupyter\n",
    "app = FastAPI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd180f10-7ee5-473f-a9ce-35fbecb192c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Whisper model loaded successfully.\n",
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\admin\\anaconda3\\anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO:__main__:Summarization pipeline loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "#---------------------Loading Models -------------------\n",
    "\n",
    "try:\n",
    "    whisper_model = whisper.load_model(\"large-v3\")  \n",
    "    logger.info(\"Whisper model loaded successfully.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading Whisper model: {str(e)}\")\n",
    "    raise RuntimeError(f\"Error loading Whisper model: {str(e)}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    summary_pipeline = pipeline(\"summarization\")\n",
    "    logger.info(\"Summarization pipeline loaded successfully.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading summarization pipeline: {str(e)}\")\n",
    "    raise RuntimeError(f\"Error loading summarization pipeline: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97cac594-8a7a-40ba-a2a6-92accad48731",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPLOAD_DIR = \"./uploads\"\n",
    "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
    "\n",
    "#---------------------saving uploaded file  ----------------\n",
    "\n",
    "def save_upload_file(upload_file: UploadFile, destination: str) -> str:\n",
    "    try:\n",
    "        with open(destination, \"wb\") as buffer:\n",
    "            buffer.write(upload_file.file.read())\n",
    "            \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"File save error: {str(e)}\")\n",
    "        \n",
    "    return destination\n",
    "\n",
    "def remove_file(file_path: str):\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82c5e031-cc80-4219-a533-b5e27b16ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------- Transcribes uploaded audio using Whisper model- ---------\n",
    "@app.post(\"/transcribe/\")\n",
    "async def transcribe_audio(file: UploadFile = File(...)):\n",
    "    file_location = os.path.join(UPLOAD_DIR, file.filename)\n",
    "    try:\n",
    "        await save_upload_file(file, file_location)\n",
    "        loop = asyncio.get_event_loop()\n",
    "        transcription_result = await loop.run_in_executor(None, whisper_model.transcribe, file_location)\n",
    "        transcription = transcription_result['text']\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Transcription error: {str(e)}\")\n",
    "        \n",
    "    finally:\n",
    "        remove_file(file_location)\n",
    "        \n",
    "    return {\"transcription\": transcription}\n",
    "\n",
    "\n",
    "#------------ Summarize the transcription of audio file--------------\n",
    "@app.post(\"/summarize/\")\n",
    "async def summarize_text(transcription: str):\n",
    "    try:\n",
    "        summary = summarizer(transcription, max_length=100, min_length=25, do_sample=False)[0]['summary_text']\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Summarization error: {str(e)}\")\n",
    "    return {\"summary\": summary}\n",
    "\n",
    "\n",
    "#------------------Extracting Time stamps from the audio file ----------\n",
    "@app.post(\"/extract-timestamps/\")\n",
    "async def extract_timestamps(file: UploadFile = File(...)):\n",
    "    file_location = os.path.join(UPLOAD_DIR, file.filename)\n",
    "    try:\n",
    "        save_upload_file(file, file_location)\n",
    "        transcription_result = whisper_model.transcribe(file_location)\n",
    "        segments = transcription_result['segments']\n",
    "        timestamps = [{\"start\": seg['start'], \"end\": seg['end'], \"text\": seg['text']} for seg in segments]\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Timestamp extraction error: {str(e)}\")\n",
    "        \n",
    "    finally:\n",
    "        remove_file(file_location)\n",
    "        \n",
    "    return {\"timestamps\": timestamps}\n",
    "\n",
    "@app.post(\"/process-audio/\")\n",
    "async def process_audio(file: UploadFile = File(...)):\n",
    "    file_location = os.path.join(UPLOAD_DIR, file.filename)\n",
    "    try:\n",
    "        save_upload_file(file, file_location)\n",
    "        transcription_result = whisper_model.transcribe(file_location)\n",
    "        transcription = transcription_result['text']\n",
    "        segments = transcription_result['segments']\n",
    "        summary = summarizer(transcription, max_length=100, min_length=25, do_sample=False)[0]['summary_text']\n",
    "        timestamps = [{\"start\": seg['start'], \"end\": seg['end'], \"text\": seg['text']} for seg in segments]\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Processing error: {str(e)}\")\n",
    "    finally:\n",
    "        remove_file(file_location)\n",
    "    return {\"transcription\": transcription, \"summary\": summary, \"timestamps\": timestamps}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cd616f-fd9c-44d5-b5f7-a1019d0edf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [15508]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "#------------ Running the Fast API application on port 8000-------------\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ffc6d-dfd5-4d08-a69a-d0b44579a002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
